{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mplsoccer import Sblocal\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change these paths/ parameters\n",
    "You will need to change these paths/ parameters depending on where the StatsBomb open-data is located, how and where you want to save the resulting data, and if you only want the new files to be processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open data folder is one folder down in the directory. To change if run elsewhere\n",
    "STATSBOMB_DATA = os.path.join('..', '..', '..', 'open-data', 'data')\n",
    "# save files in folder in current directory. To change if want to save elsewhere\n",
    "DATA_FOLDER = os.path.join('..', '..', 'data', 'statsbomb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the data file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_links = glob.glob(os.path.join(STATSBOMB_DATA, 'events', '**', '*.json'), recursive=True)\n",
    "lineup_links = glob.glob(os.path.join(STATSBOMB_DATA, 'lineups', '**', '*.json'), recursive=True)\n",
    "match_links = glob.glob(os.path.join(STATSBOMB_DATA, 'matches', '**', '*.json'), recursive=True)\n",
    "competition_path = os.path.join(STATSBOMB_DATA, 'competitions.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make the directory structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../data/statsbomb/event_raw'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/b5/p9rmdc817y36m0hgtkqmx93m0000gn/T/ipykernel_93972/3485913147.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_FOLDER\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfolder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../data/statsbomb/event_raw'"
     ]
    }
   ],
   "source": [
    "# make the directory structure\n",
    "for folder in ['event_raw', 'related_raw', 'freeze_raw', 'tactic_raw', 'lineup_raw']:\n",
    "    path = os.path.join(DATA_FOLDER, folder)\n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create StatsBomb Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = Sblocal()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the competition data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_competition = parser.competition(competition_path)\n",
    "df_competition.to_parquet(os.path.join(DATA_FOLDER, 'competition.parquet'), allow_truncated_timestamps=True)\n",
    "df_competition.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keep a copy of the match dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_path = os.path.join(DATA_FOLDER, 'match.parquet')\n",
    "if os.path.exists(match_path):\n",
    "    df_match_copy = pd.read_parquet(match_path).copy()\n",
    "    UPDATE_FILES = True\n",
    "else:\n",
    "    UPDATE_FILES = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the match data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "match_dfs = [parser.match(file) for file in match_links]\n",
    "df_match = pd.concat(match_dfs)\n",
    "# again there is a slight loss of quality when saving timestamps, but only relevant for last_updated\n",
    "df_match.to_parquet(os.path.join(DATA_FOLDER, 'match.parquet'),\n",
    "                    allow_truncated_timestamps=True)\n",
    "df_match.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check which games have been updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if UPDATE_FILES:\n",
    "    df_match_copy = (df_match[['match_id', 'last_updated']]\n",
    "                     .merge(df_match_copy[['match_id', 'last_updated']],\n",
    "                            how='left', suffixes=['', '_old'], on='match_id'))\n",
    "    df_match_copy = df_match_copy[(df_match_copy.last_updated.dt.floor('ms') !=\n",
    "                                   df_match_copy.last_updated_old.dt.floor('ms'))].copy()\n",
    "    to_update = df_match_copy.match_id.unique()\n",
    "\n",
    "    # get array of event links to update - based on whether they have been updated in the match json\n",
    "    event_link_ids = [int(os.path.splitext(os.path.basename(link))[0]) for link in event_links]\n",
    "    event_to_update = [link in to_update for link in event_link_ids]\n",
    "    event_links = np.array(event_links)[event_to_update]\n",
    "\n",
    "    # get array of lineup links to update -\n",
    "    # based on whether they have been updated in the match jsons\n",
    "    lineup_link_ids = [int(os.path.splitext(os.path.basename(link))[0]) for link in lineup_links]\n",
    "    lineup_to_update = [link in to_update for link in lineup_link_ids]\n",
    "    lineup_links = np.array(lineup_links)[lineup_to_update]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the lineup data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LINEUP_FOLDER = os.path.join(DATA_FOLDER, 'lineup_raw')\n",
    "# loop through all the changed links and store as parquet files - small and fast files\n",
    "for file in tqdm(lineup_links):\n",
    "    save_path = f'{os.path.basename(file)[:-4]}parquet'\n",
    "    try:\n",
    "        df_lineup = parser.lineup(file)\n",
    "        df_lineup.to_parquet(os.path.join(LINEUP_FOLDER, save_path))\n",
    "    except ValueError:\n",
    "        print('Skipping file:', file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert to a single dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# players with split ids\n",
    "to_replace = {18103: 38522}  # Dietmar Hamann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(lineup_links) == 0:\n",
    "    print('No update')\n",
    "else:\n",
    "    lineup_files = glob.glob(os.path.join(LINEUP_FOLDER, '*.parquet'))\n",
    "    df_lineup = pd.concat([pd.read_parquet(file) for file in tqdm(lineup_files)])\n",
    "    # replace some ids that appear to be split\n",
    "    df_lineup.player_id.replace(to_replace, inplace=True)\n",
    "    df_lineup.to_parquet(os.path.join(DATA_FOLDER, 'lineup.parquet'))\n",
    "    df_lineup.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the event data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# loop through all the changed links and store as parquet files - small and fast files\n",
    "for file in tqdm(event_links):\n",
    "    save_path = f'{os.path.basename(file)[:-4]}parquet'\n",
    "    try:\n",
    "        events, related, freeze, tactics = parser.event(file)\n",
    "        # save to parquet files\n",
    "        # using the dictionary key to access the dataframes from the dictionary\n",
    "        events.to_parquet(os.path.join(DATA_FOLDER, 'event_raw', save_path))\n",
    "        related.to_parquet(os.path.join(DATA_FOLDER, 'related_raw', save_path))\n",
    "        freeze.to_parquet(os.path.join(DATA_FOLDER, 'freeze_raw', save_path))\n",
    "        tactics.to_parquet(os.path.join(DATA_FOLDER, 'tactic_raw', save_path))\n",
    "    except ValueError:\n",
    "        print('Skipping:', file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If updating the event dataframe get a list of ids to update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_files = glob.glob(os.path.join(DATA_FOLDER, 'event_raw', '*.parquet'))\n",
    "if UPDATE_FILES:\n",
    "    ids_to_update = [int(os.path.splitext(os.path.basename(link))[0]) for link in event_links]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to load old dataframe and combine with updated parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(directory, file_type, update_ids):\n",
    "    \"\"\" Update an old DataFrame with files that have changed/ been added.\n",
    "    Parameters\n",
    "    ----------\n",
    "    directory : path to directory containing the files\n",
    "    file_type : str\n",
    "        One of 'event', 'freeze', 'tatic', or related'\n",
    "    update_ids : list of integers\n",
    "        A list of the match ids to update\n",
    "    Returns\n",
    "    -------\n",
    "    df : pandas.DataFrame\n",
    "        An updated DataFrame with the new/changed matches.\n",
    "    \"\"\"\n",
    "    # get a list of parquet files to add to the old dataframe\n",
    "    files = glob.glob(os.path.join(directory, f'{file_type}_raw', '*.parquet'))\n",
    "    files_id = [int(os.path.splitext(os.path.basename(file))[0]) for file in files]\n",
    "    mask_update = [match_id in update_ids for match_id in files_id]\n",
    "    files = np.array(files)[mask_update]\n",
    "    # load the old dataframe, filter out changed matches and add the new parquet files\n",
    "    df_old = pd.read_parquet(os.path.join(directory, f'{file_type}.parquet'))\n",
    "    df_old = df_old[~df_old.match_id.isin(update_ids)]\n",
    "    df_new = pd.concat([pd.read_parquet(file) for file in tqdm(files)])\n",
    "    df_old = pd.concat([df_old, df_new])\n",
    "    return df_old"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single dataframe events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(event_links) == 0:\n",
    "    print('No update')\n",
    "else:\n",
    "    if UPDATE_FILES:\n",
    "        df_event = update(DATA_FOLDER, 'event', ids_to_update)\n",
    "        df_event.player_id.replace(to_replace, inplace=True)  # replace some ids that appear to be split\n",
    "        df_event.to_parquet(os.path.join(DATA_FOLDER, 'event.parquet'))\n",
    "        df_event.info(verbose=True, show_counts=True)\n",
    "    else:\n",
    "        df_event = pd.concat([pd.read_parquet(file) for file in tqdm(event_files)])\n",
    "        df_event.player_id.replace(to_replace, inplace=True)  # replace some ids that appear to be split\n",
    "        df_event.to_parquet(os.path.join(DATA_FOLDER, 'event.parquet'))\n",
    "        df_event.info(verbose=True, show_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single dataframe shot freeze frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(event_links) == 0:\n",
    "    print('No update')\n",
    "else:\n",
    "    if UPDATE_FILES:\n",
    "        df_freeze = update(DATA_FOLDER, 'freeze', ids_to_update)\n",
    "        df_freeze.player_id.replace(to_replace, inplace=True)  # replace some ids that appear to be split\n",
    "        df_freeze.to_parquet(os.path.join(DATA_FOLDER, 'freeze.parquet'))\n",
    "        df_freeze.info(verbose=True, show_counts=True)\n",
    "    else:\n",
    "        freeze_files = glob.glob(os.path.join(DATA_FOLDER, 'freeze_raw', '*.parquet'))\n",
    "        df_freeze = pd.concat([pd.read_parquet(file) for file in tqdm(freeze_files)])\n",
    "        df_freeze.player_id.replace(to_replace, inplace=True)  # replace some ids that appear to be split\n",
    "        df_freeze.to_parquet(os.path.join(DATA_FOLDER, 'freeze.parquet'))\n",
    "        df_freeze.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single dataframe tactics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(event_links) == 0:\n",
    "    print('No update')\n",
    "else:\n",
    "    if UPDATE_FILES:\n",
    "        df_tactic = update(DATA_FOLDER, 'tactic', ids_to_update)\n",
    "        df_tactic.player_id.replace(to_replace, inplace=True)  # replace some ids that appear to be split\n",
    "        df_tactic.to_parquet(os.path.join(DATA_FOLDER, 'tactic.parquet'))\n",
    "        df_tactic.info(verbose=True, show_counts=True)\n",
    "    else:\n",
    "        tactic_files = glob.glob(os.path.join(DATA_FOLDER, 'tactic_raw', '*.parquet'))\n",
    "        df_tactic = pd.concat([pd.read_parquet(file) for file in tqdm(tactic_files)])\n",
    "        df_tactic.player_id.replace(to_replace, inplace=True)  # replace some ids that appear to be split\n",
    "        df_tactic.to_parquet(os.path.join(DATA_FOLDER, 'tactic.parquet'))\n",
    "        df_tactic.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single dataframe related events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(event_links) == 0:\n",
    "    print('No update')\n",
    "else:\n",
    "    if UPDATE_FILES:\n",
    "        df_related = update(DATA_FOLDER, 'related', ids_to_update)\n",
    "        df_related.to_parquet(os.path.join(DATA_FOLDER, 'related.parquet'))\n",
    "        df_related.info(verbose=True, show_counts=True)\n",
    "    else:\n",
    "        related_files = glob.glob(os.path.join(DATA_FOLDER, 'related_raw', '*.parquet'))\n",
    "        df_related = pd.concat([pd.read_parquet(file) for file in tqdm(related_files)])\n",
    "        df_related.to_parquet(os.path.join(DATA_FOLDER, 'related.parquet'))\n",
    "        df_related.info(verbose=True, show_counts=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
